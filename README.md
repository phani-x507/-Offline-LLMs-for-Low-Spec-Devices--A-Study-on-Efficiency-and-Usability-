# Offline LLMs for Low-Spec Devices: A  Study on Efficiency and Usability 
 The rapid advancements in Offline Large Language Models (LLMs) have  paved the way for efficient and privacy-focused AI applications. This project presents a  comprehensive benchmarking and evaluation framework for Masked Language  Models (MLMs) and Causal Language Models (CLMs) running entirely offline. The  study aims to analyze model performance, efficiency, and accuracy across multiple  locally stored models, including DistilBERT, MobileBERT, TinyLLaMA, Phi-2, and  Gemma-2B.  To achieve this, we implement a systematic evaluation pipeline that measures model  accuracy and generation quality using industry-standard NLP metrics such as BLEU,  ROUGE, and Perplexity. The benchmarking framework ensures robust testing by  handling model-specific tokenization issues, enabling quantization for memory  efficiency, and adapting padding strategies for CLMs like Phi-2. Additionally, this  project highlights the feasibility of running LLMs without internet dependency, making  them ideal for privacy-sensitive or resource-constrained environments.  The results offer valuable insights into the trade-offs between model size, inference  speed, and text generation quality. This evaluation framework serves as a foundation  for selecting the most optimal LLM for offline applications, such as AI-powered  chatbots, document summarization, and intelligent assistants in secure environments. 
